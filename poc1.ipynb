{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import List, Optional\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person.\")\n",
    "    birth_date: Optional[str] = Field(default=None, description=\"The person's birth date\")\n",
    "\n",
    "\n",
    "class Model(BaseModel):\n",
    "    \n",
    "    persons: Optional[List[Person]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parser\n",
    "parser = PydanticOutputParser(pydantic_object=Model)\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the user query. Wrap the output in ```json and ``` tags\\n{format_instructions}. Do not provide more fields that the one required.\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | llm | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(persons=[Person(name='Emanuel Burckhardt', birth_date='25.11.1744'), Person(name='Alfred Abion', birth_date='1st September 2021')])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial_text =  \"Emanuel Burckhardt est né le 25.11.1744 à Bâle, et mort le 21.5.1820 à Palerme, protestant. Her husband is named Aurelius and was born on 1st September 1999.\"\n",
    "initial_text = \"Emanuel Burckhardt naît le 25.11.1744 à Bâle, meurt le 21.5.1820 à Palerme, protestant, de Bâle. \" \\\n",
    "               \"Fils de John Doe, lieutenant au service de France. \" \\\n",
    "               \"Marié(e) (1783) à Theresia Münster, de Bâle. \" \\\n",
    "               \"Alfred Abion was born on 1st September 2021.\"\n",
    "               #\"Au service de France jusqu'en 1787, puis de Naples. \" \\\n",
    "               #\"Commandant en chef des troupes du royaume de Sicile (1802), capitaine général des troupes du royaume des Deux-Siciles (1815). \" \\\n",
    "               #\"Emanuel Burckhardt dut sa carrière à ses compétences et à sa loyauté envers Ferdinand IV.\"\n",
    "\n",
    "chain.invoke({\"text\": initial_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL TEXT ===\n",
      "Emanuel Burckhardt naît le 25.11.1744 à Bâle, meurt le 21.5.1820 à Palerme, protestant, de Bâle. Fils de John Doe, lieutenant au service de France. Marié(e) (1783) à Theresia Münster, de Bâle. \n",
      "=== PROMPT ===\n",
      "Answer the user query. Wrap the output in ```json and ``` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"persons\": {\"title\": \"Persons\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person.\", \"type\": \"string\"}, \"birth_date\": {\"title\": \"Birth Date\", \"description\": \"The person's birth date\", \"type\": \"string\"}}}}}\n",
      "```. Do not provide more fields that the one required.\n",
      "---\n",
      "Emanuel Burckhardt naît le 25.11.1744 à Bâle, meurt le 21.5.1820 à Palerme, protestant, de Bâle. Fils de John Doe, lieutenant au service de France. Marié(e) (1783) à Theresia Münster, de Bâle. \n",
      "---\n",
      "=== LLM ANSWER ===\n",
      "```json\n",
      "{\n",
      "  \"persons\": [\n",
      "    {\n",
      "      \"name\": \"Emanuel Burckhardt\",\n",
      "      \"birth_date\": \"25.11.1744\",\n",
      "      \"death_date\": \"21.5.1820\",\n",
      "      \"religion\": \"protestant\",\n",
      "      \"nationality\": \"de Bâle\",\n",
      "      \"father\": \"John Doe\",\n",
      "      \"mother\": \"\",\n",
      "      \"spouse\": \"Theresia Münster\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "=== PARSED ANSWER ===\n",
      "persons=[Person(name='Emanuel Burckhardt', birth_date='25.11.1744')]\n"
     ]
    }
   ],
   "source": [
    "def analyze_chain(text):\n",
    "\n",
    "    print('=== INITIAL TEXT ===')\n",
    "    print(text)\n",
    "\n",
    "    prompt_result = prompt.invoke({\"text\": initial_text})\n",
    "    print('=== PROMPT ===')\n",
    "    for message in prompt_result.messages:\n",
    "        print(message.content)\n",
    "        print('---')\n",
    "\n",
    "    llm_result = llm.invoke(prompt_result)\n",
    "    print('=== LLM ANSWER ===')\n",
    "    print(llm_result.content)\n",
    "\n",
    "    parser_result = parser.invoke(llm_result)\n",
    "    print('=== PARSED ANSWER ===')\n",
    "    print(parser_result)\n",
    "\n",
    "\n",
    "analyze_chain(initial_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to LLM:\n",
      "Answer the user query. Wrap the output in ```json and ``` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"persons\": {\"title\": \"Persons\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person.\", \"type\": \"string\"}, \"birth_date\": {\"title\": \"Birth Date\", \"description\": \"The person's birth date\", \"type\": \"string\"}}}}}\n",
      "```\n",
      "Emanuel Burckhardt est né le 25.11.1744 à Bâle, et mort le 21.5.1820 à Palerme, protestant. Her husband is named Aurelius and was born on 1st September 1999.\n",
      "\n",
      "Raw LLM Response:\n",
      "content='```json\\n{\\n    \"persons\": [\\n        {\\n            \"name\": \"Emanuel Burckhardt\",\\n            \"birth_date\": \"25.11.1744\"\\n        },\\n        {\\n            \"name\": \"Aurelius\",\\n            \"birth_date\": \"1st September 1999\"\\n        }\\n    ]\\n}\\n```' response_metadata={'model': 'llama3.1', 'created_at': '2024-09-06T13:44:18.629028Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3743783416, 'load_duration': 29938833, 'prompt_eval_count': 317, 'prompt_eval_duration': 1121768000, 'eval_count': 68, 'eval_duration': 2591083000} id='run-897f8749-f0f3-4552-a1dc-98ef2cdee846-0' usage_metadata={'input_tokens': 317, 'output_tokens': 68, 'total_tokens': 385}\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mrun_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mrun_chain\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw LLM Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mllm_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Parse the response\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m parsed_response \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsed Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mparsed_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/output_parsers/pydantic.py:82\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:98\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m        The parsed JSON object.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define the parser\n",
    "parser = PydanticOutputParser(pydantic_object=Model)\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "# Define the prompt template\n",
    "system_message = \"Answer the user query. Wrap the output in ```json and ``` tags\\n{format_instructions}\"\n",
    "human_message = \"{text}\"\n",
    "\n",
    "def run_chain(text):\n",
    "    # Manually create the formatted prompt\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    full_prompt = system_message.format(format_instructions=format_instructions) + \"\\n\" + human_message.format(text=text)\n",
    "    \n",
    "    print(f\"Input to LLM:\\n{full_prompt}\\n\")\n",
    "    \n",
    "    # Get the LLM response\n",
    "    llm_response = llm.invoke(full_prompt)  # Directly call the LLM\n",
    "    print(f\"Raw LLM Response:\\n{llm_response}\\n\")\n",
    "    \n",
    "    # Parse the response\n",
    "    parsed_response = parser.parse(llm_response)\n",
    "    print(f\"Parsed Response:\\n{parsed_response}\\n\")\n",
    "    \n",
    "    return parsed_response\n",
    "\n",
    "# Example usage\n",
    "run_chain(initial_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Morgane was born in Mulhouse in 1998, and Gaétan in Sierentz in 1888.\"\"\"\n",
    "chain.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in ```json and `` tags\\n{format_instructions}\"\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
